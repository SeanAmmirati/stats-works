---
title: "t_test_bayes_example"
author: "Sean Ammirati"
date: "July 29, 2018"
output: html_document
---
```{r}
source('./t_test_bayes_functions.R')
```

# Standard T-Test
Consider the t-test of equality of means. Using the standard procedure for a t-test: 
```{r}
vec1 <- c(120,107,110,116,114,111,113,117,114,112)
vec2 <- c(110,111,107,108,110,105,107,106,111,111)

t_test_res = test_equality_means(vec1, vec2)
t_test_res
```

We can see that the t-test gives us a significant result -- the two means would have a difference smaller than the observed difference 
with probability `r t_test_res$prob`. We then would reject the null hypothesis with 95% confidence (or even 99% confidence.)

#Bayesian Approach

Now let's try to solve this problem with our Bayesian hats! Assuming that we have a prior distribution of $\mu$ and $\sigma^2$ such that 
$x \mid \mu, \sigma^2 \sim N(\mu, \sigma^2)$ , we want the posterior $f(\mu \mid x_1, x_2, ..., x_n, \sigma^2)$ . Assuming a prior of $\mu \mid \sigma^2 \sim N(\beta, \frac{\sigma}{\sqrt{n}})$ and 
$\frac{S}{\sigma^2} \sim \chi^2(k)$, we start with prior parameters $\beta_0, n, S_0, k$  and update for each value of x. 


```{r}
simulatepostprob(vec = vec1) - simulatepostprob(vec = vec2)
```
By iterating for each observation, we arrive at a posterior distribution for $\mu \mid \sigma^2$ and $\frac{S}{\sigma^2}$. We then sample randomly from these theoretical distributions to 
sample from the posterior distributions. If we replicate this many times, we will be able to estimate the differences between the posterior distributions of the two vectors.

Using uninformative prior gives similar results to frequentist t-test method
```{r}
sum(rpost(1000,vec1) > rpost(1000,vec2))/1000
```

We can then use our prior information about the sample to produce a mixed, updated result. For instance, with prior parameters: 

```{r}
prior_params = c(n=10, S=2, k=9, B=30)
```
We can see our posterior: 

```{r}
sum(rpost(1000, vec1, prior_params) > rpost(1000, vec2, prior_params))/1000
```